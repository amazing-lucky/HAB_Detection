{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHKCUrfASwNOZPhaUOT16P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amazing-lucky/HAB_Detection/blob/main/Synthethic_HAB_GAN_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryrjwelBSeR3",
        "outputId": "75371ab9-fe88-4c34-def4-44f64b739831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDN71XHuBq4G",
        "outputId": "d2ace5c6-06f5-4174-d7bf-0817c06f4230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GAN training...\n",
            "Epoch [500/5000], d_loss: 0.9936, g_loss: 1.5032\n",
            "Epoch [1000/5000], d_loss: 0.2025, g_loss: 4.0647\n",
            "Epoch [1500/5000], d_loss: 0.0996, g_loss: 6.7787\n",
            "Epoch [2000/5000], d_loss: 0.1536, g_loss: 9.1018\n",
            "Epoch [2500/5000], d_loss: 0.0045, g_loss: 8.9463\n",
            "Epoch [3000/5000], d_loss: 0.2497, g_loss: 5.0255\n",
            "Epoch [3500/5000], d_loss: 0.0529, g_loss: 4.1512\n",
            "Epoch [4000/5000], d_loss: 0.0657, g_loss: 4.3284\n",
            "Epoch [4500/5000], d_loss: 0.1239, g_loss: 4.1466\n",
            "Epoch [5000/5000], d_loss: 0.4182, g_loss: 3.4691\n",
            "GAN training complete!\n",
            "\n",
            "Synthetic Data Summary:\n",
            "       Bloom_Index  Rolling_Chlorophyll_Anomaly  Rolling_SST_Anomaly  \\\n",
            "count  2000.000000                  2000.000000          2000.000000   \n",
            "mean     -0.067774                     1.456813            -0.722058   \n",
            "std       0.392716                     1.157638             1.178126   \n",
            "min      -0.876400                    -1.771426            -2.919351   \n",
            "25%      -0.288655                     0.482426            -1.433621   \n",
            "50%      -0.235182                     1.538880            -0.517914   \n",
            "75%       0.098498                     2.017061             0.188812   \n",
            "max       0.913177                     4.679882             2.618255   \n",
            "\n",
            "       Surface_Chlorophyll  Sea_Surface_Temperature  Dissolved_Oxygen  \\\n",
            "count          2000.000000              2000.000000       2000.000000   \n",
            "mean              8.738934                25.057344          2.324451   \n",
            "std               1.607372                 3.639903          1.753652   \n",
            "min               4.142816                20.013662          0.000000   \n",
            "25%               6.969511                22.159429          0.079895   \n",
            "50%               9.846098                24.071608          2.875928   \n",
            "75%              10.000000                28.017788          3.481411   \n",
            "max              10.000000                34.811722          6.884605   \n",
            "\n",
            "                pH  Total_Nitrogen  Total_Phosphorus  HAB_Present  \n",
            "count  2000.000000     2000.000000       2000.000000  2000.000000  \n",
            "mean      8.728385        3.591766          0.576667     0.166500  \n",
            "std       0.635536        1.860744          0.175703     0.372622  \n",
            "min       7.325528        0.216421          0.031508     0.000000  \n",
            "25%       8.023291        2.340877          0.412113     0.000000  \n",
            "50%       8.843324        3.583943          0.624404     0.000000  \n",
            "75%       9.295158        3.742746          0.666852     0.000000  \n",
            "max       9.500000        9.537274          0.977449     1.000000  \n",
            "\n",
            "HAB Distribution:\n",
            "No HAB: 1667\n",
            "HAB Present: 333\n",
            "\n",
            "Synthetic data saved to 'synthetic_hab_data_gan.csv'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define parameter ranges\n",
        "param_ranges = {\n",
        "    'Bloom_Index': (-1.0, 1.0),\n",
        "    'Rolling_Chlorophyll_Anomaly': (-2.0, 5.0),\n",
        "    'Rolling_SST_Anomaly': (-3.0, 3.0),\n",
        "    'Surface_Chlorophyll': (0.0, 10.0),\n",
        "    'Sea_Surface_Temperature': (20.0, 35.0),\n",
        "    'Dissolved_Oxygen': (0.0, 12.0),\n",
        "    'pH': (6.5, 9.5),\n",
        "    'Total_Nitrogen': (0.0, 10.0),\n",
        "    'Total_Phosphorus': (0.0, 1.0)\n",
        "}\n",
        "\n",
        "# Number of parameters\n",
        "num_params = len(param_ranges)\n",
        "\n",
        "# Define Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_params),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "# Define Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(num_params, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Create a small seed dataset with realistic correlations\n",
        "def create_seed_dataset(n_samples=500):\n",
        "    data = {}\n",
        "\n",
        "    # Generate base random values\n",
        "    for param, (min_val, max_val) in param_ranges.items():\n",
        "        data[param] = np.random.uniform(min_val, max_val, n_samples)\n",
        "\n",
        "    # Add correlations between variables\n",
        "    data['pH'] += data['Surface_Chlorophyll'] * 0.05\n",
        "    data['Dissolved_Oxygen'] -= data['Surface_Chlorophyll'] * 0.2\n",
        "    data['Dissolved_Oxygen'] -= (data['Sea_Surface_Temperature'] - 20) * 0.1\n",
        "    data['Surface_Chlorophyll'] += data['Total_Nitrogen'] * 0.2 + data['Total_Phosphorus'] * 1.5\n",
        "\n",
        "    # Clip values to ensure they stay within ranges\n",
        "    for param, (min_val, max_val) in param_ranges.items():\n",
        "        data[param] = np.clip(data[param], min_val, max_val)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Create seed dataset\n",
        "seed_df = create_seed_dataset()\n",
        "\n",
        "# Normalize data to [-1, 1] range for GAN training\n",
        "def normalize_data(df):\n",
        "    normalized_df = df.copy()\n",
        "    for param, (min_val, max_val) in param_ranges.items():\n",
        "        normalized_df[param] = 2 * (df[param] - min_val) / (max_val - min_val) - 1\n",
        "    return normalized_df\n",
        "\n",
        "# Denormalize data back to original ranges\n",
        "def denormalize_data(df):\n",
        "    denormalized_df = df.copy()\n",
        "    for param, (min_val, max_val) in param_ranges.items():\n",
        "        denormalized_df[param] = (df[param] + 1) * (max_val - min_val) / 2 + min_val\n",
        "    return denormalized_df\n",
        "\n",
        "# Normalize seed data\n",
        "normalized_seed_df = normalize_data(seed_df)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "real_data = torch.FloatTensor(normalized_seed_df.values)\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(real_data)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5000\n",
        "print(\"Starting GAN training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader):\n",
        "        batch_size = data[0].size(0)\n",
        "\n",
        "        # Real data\n",
        "        real_data_batch = data[0]\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "\n",
        "        # Fake data\n",
        "        z = torch.randn(batch_size, 100)\n",
        "        fake_data_batch = generator(z)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "        # Train discriminator\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        # Real data loss\n",
        "        real_outputs = discriminator(real_data_batch)\n",
        "        d_loss_real = criterion(real_outputs, real_labels)\n",
        "\n",
        "        # Fake data loss\n",
        "        fake_outputs = discriminator(fake_data_batch.detach())\n",
        "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train generator\n",
        "        generator.zero_grad()\n",
        "        fake_outputs = discriminator(fake_data_batch)\n",
        "        g_loss = criterion(fake_outputs, real_labels)\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
        "\n",
        "print(\"GAN training complete!\")\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_synthetic_data(num_samples=2000):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_samples, 100)\n",
        "        synthetic_data = generator(z).numpy()\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    synthetic_df = pd.DataFrame(synthetic_data, columns=param_ranges.keys())\n",
        "\n",
        "    # Denormalize to original ranges\n",
        "    synthetic_df = denormalize_data(synthetic_df)\n",
        "\n",
        "    # Apply correlations to ensure relationships are maintained\n",
        "    synthetic_df['pH'] += synthetic_df['Surface_Chlorophyll'] * 0.05\n",
        "    synthetic_df['Dissolved_Oxygen'] -= synthetic_df['Surface_Chlorophyll'] * 0.2\n",
        "    synthetic_df['Dissolved_Oxygen'] -= (synthetic_df['Sea_Surface_Temperature'] - 20) * 0.1\n",
        "    synthetic_df['Surface_Chlorophyll'] += synthetic_df['Total_Nitrogen'] * 0.2 + synthetic_df['Total_Phosphorus'] * 1.5\n",
        "\n",
        "    # Clip values to ensure they stay within ranges\n",
        "    for param, (min_val, max_val) in param_ranges.items():\n",
        "        synthetic_df[param] = np.clip(synthetic_df[param], min_val, max_val)\n",
        "\n",
        "    # Add HAB_Present column based on thresholds\n",
        "    synthetic_df['HAB_Present'] = ((synthetic_df['Bloom_Index'] > 0.2) &\n",
        "                                  (synthetic_df['Rolling_Chlorophyll_Anomaly'] > 1.0) &\n",
        "                                  (synthetic_df['Surface_Chlorophyll'] > 3.0)).astype(int)\n",
        "\n",
        "    return synthetic_df\n",
        "\n",
        "# Generate 2000 synthetic samples\n",
        "synthetic_data = generate_synthetic_data(2000)\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\nSynthetic Data Summary:\")\n",
        "print(synthetic_data.describe())\n",
        "\n",
        "# Display HAB distribution\n",
        "hab_count = synthetic_data['HAB_Present'].value_counts()\n",
        "print(f\"\\nHAB Distribution:\\nNo HAB: {hab_count.get(0, 0)}\\nHAB Present: {hab_count.get(1, 0)}\")\n",
        "\n",
        "# Save to CSV\n",
        "synthetic_data.to_csv('synthetic_hab_data_gan.csv', index=False)\n",
        "print(\"\\nSynthetic data saved to 'synthetic_hab_data_gan.csv'\")\n"
      ]
    }
  ]
}